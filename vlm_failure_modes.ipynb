{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VLM Failure Modes â€” Self-Contained Colab Execution\n",
                "\n",
                "This notebook runs the VLM failure modes experiments entirely within the Colab runtime.\n",
                "No Google Drive mounting is required. All necessary code is written to the environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment & Install LLaVA\n",
                "!git clone https://github.com/haotian-liu/LLaVA.git\n",
                "!pip install --upgrade pip\n",
                "!pip install -e LLaVA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Install Project Dependencies\n",
                "%%writefile requirements.txt\n",
                "# Core ML\n",
                "torch\n",
                "torchvision\n",
                "torchaudio\n",
                "\n",
                "# HuggingFace / VLM\n",
                "transformers>=4.36.0\n",
                "accelerate\n",
                "sentencepiece\n",
                "\n",
                "# LLaVA deps\n",
                "einops\n",
                "timm\n",
                "opencv-python\n",
                "pillow\n",
                "scipy\n",
                "numpy\n",
                "tqdm\n",
                "pyyaml\n",
                "\n",
                "# Optional (GPU efficiency on Colab)\n",
                "bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Create Project Structure\n",
                "import os\n",
                "os.makedirs(\"probes\", exist_ok=True)\n",
                "os.makedirs(\"attacks\", exist_ok=True)\n",
                "os.makedirs(\"experiments\", exist_ok=True)\n",
                "os.makedirs(\"results\", exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Write Code: Probes (Entropy)\n",
                "%%writefile probes/entropy.py\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "def token_entropy(logits):\n",
                "    # Ensure logits are float32 for stability\n",
                "    logits = logits.to(torch.float32)\n",
                "    probs = F.softmax(logits, dim=-1)\n",
                "    entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1)\n",
                "    return entropy.mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Write Code: Attacks (PGD Visual)\n",
                "%%writefile attacks/pgd_visual.py\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "def pgd_attack(model, images, input_ids, epsilon=0.1, alpha=0.01, num_iter=20):\n",
                "    \"\"\"\n",
                "    Projected Gradient Descent (PGD) attack on visual inputs.\n",
                "    Objective: Diverge from the original model predictions (Maximize KL Divergence).\n",
                "    \n",
                "    Args:\n",
                "        model: VLM model\n",
                "        images: Preprocessed image tensor [B, C, H, W]\n",
                "        input_ids: Tokenized input prompt\n",
                "        epsilon: Maximum perturbation magnitude\n",
                "        alpha: Step size\n",
                "        num_iter: Number of iterations\n",
                "        \n",
                "    Returns:\n",
                "        perturbed_images: Adversarial image tensor\n",
                "    \"\"\"\n",
                "    # Clone and detach\n",
                "    perturbed_images = images.clone().detach()\n",
                "    perturbed_images.requires_grad = True\n",
                "    \n",
                "    # Get clean logits (Target to diverge from)\n",
                "    with torch.no_grad():\n",
                "        clean_outputs = model(input_ids, images=images)\n",
                "        clean_logits = clean_outputs.logits\n",
                "        \n",
                "    # Optimization loop\n",
                "    for _ in range(num_iter):\n",
                "        perturbed_images.requires_grad = True\n",
                "        \n",
                "        # Forward pass\n",
                "        outputs = model(input_ids, images=perturbed_images)\n",
                "        logits = outputs.logits\n",
                "        \n",
                "        # Loss: KL Divergence (Unreduced first to handle batches properly if needed, but reduction batchmean is fine)\n",
                "        # We want to maximize the distance between clean and perturbed distributions.\n",
                "        # F.kl_div(input, target) expects input to be log-probs.\n",
                "        loss = F.kl_div(\n",
                "            F.log_softmax(logits, dim=-1),\n",
                "            F.softmax(clean_logits, dim=-1),\n",
                "            reduction='batchmean'\n",
                "        )\n",
                "        \n",
                "        # Gradient Ascent: Maximize KL Divergence\n",
                "        grad = torch.autograd.grad(loss, perturbed_images)[0]\n",
                "        \n",
                "        # Update image\n",
                "        perturbed_images = perturbed_images.detach() + alpha * grad.sign()\n",
                "        \n",
                "        # Project back to epsilon ball (L-inf)\n",
                "        delta = torch.clamp(perturbed_images - images, -epsilon, epsilon)\n",
                "        \n",
                "        # Clamp to valid range (Approximate 0-1 range logic, though images are normalized)\n",
                "        # Ideally we should un-normalize, clamp, and re-normalize, but simpler clamping prevents explosion.\n",
                "        # Using original min/max as bounds ensures we don't drift too far from valid pixel space scale.\n",
                "        perturbed_images = torch.clamp(images + delta, images.min(), images.max()).detach()\n",
                "        \n",
                "    return perturbed_images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Write Code: Sanity Check\n",
                "%%writefile experiments/sanity_check.py\n",
                "from llava.model.builder import load_pretrained_model\n",
                "from llava.mm_utils import get_model_name_from_path\n",
                "\n",
                "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
                "model_name = get_model_name_from_path(model_path)\n",
                "\n",
                "print(\"Starting model load (this may take time)...\")\n",
                "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
                "    model_path, \n",
                "    None, \n",
                "    model_name,\n",
                "    device_map=\"auto\",\n",
                "    offload_folder=\"offload\"  # Handle offloading for memory-constrained environments\n",
                ")\n",
                "\n",
                "print(\"Model loaded successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Write Code: Entropy Analysis\n",
                "%%writefile experiments/entropy_analysis.py\n",
                "import torch\n",
                "from PIL import Image\n",
                "import os\n",
                "from llava.model.builder import load_pretrained_model\n",
                "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
                "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
                "from probes.entropy import token_entropy\n",
                "from attacks.pgd_visual import pgd_attack\n",
                "\n",
                "# Configuration\n",
                "MODEL_PATH = \"liuhaotian/llava-v1.5-7b\"\n",
                "IMAGE_PATH = \"LLaVA/images/llava_logo.png\"\n",
                "PROMPT = \"Describe this image in detail.\"\n",
                "\n",
                "def main():\n",
                "    print(\"Loading model...\")\n",
                "    model_name = get_model_name_from_path(MODEL_PATH)\n",
                "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
                "        MODEL_PATH, \n",
                "        None, \n",
                "        model_name,\n",
                "        device_map=\"auto\",\n",
                "        offload_folder=\"offload\"\n",
                "    )\n",
                "    # Ensure model is in eval mode\n",
                "    model.eval()\n",
                "\n",
                "    # Load & Preprocess Image\n",
                "    print(f\"Loading image from {IMAGE_PATH}\")\n",
                "    image = Image.open(IMAGE_PATH).convert('RGB')\n",
                "    image_tensor = process_images([image], image_processor, model.config)\n",
                "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
                "\n",
                "    # Prepare Prompt\n",
                "    if model.config.mm_use_im_start_end:\n",
                "        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + PROMPT\n",
                "    else:\n",
                "        qs = DEFAULT_IMAGE_TOKEN + '\\n' + PROMPT\n",
                "\n",
                "    input_ids = tokenizer_image_token(qs, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
                "\n",
                "    # 1. Clean Baseline\n",
                "    print(\"Running Clean Forward Pass...\")\n",
                "    with torch.no_grad():\n",
                "        clean_outputs = model(input_ids, images=image_tensor)\n",
                "        clean_entropy = token_entropy(clean_outputs.logits)\n",
                "    print(f\"Clean Entropy: {clean_entropy:.4f}\")\n",
                "\n",
                "    # 2. Adversarial Attack\n",
                "    print(\"Running PGD Attack (Visual)...\")\n",
                "    # PGD attack\n",
                "    print(f\"  Starting PGD (1 iteration)...\")\n",
                "    adv_image_tensor = pgd_attack(model, image_tensor, input_ids, epsilon=0.1, alpha=0.01, num_iter=1)\n",
                "\n",
                "    # 3. Adversarial Pass\n",
                "    print(\"Running Adversarial Forward Pass...\")\n",
                "    with torch.no_grad():\n",
                "        adv_outputs = model(input_ids, images=adv_image_tensor)\n",
                "        adv_entropy = token_entropy(adv_outputs.logits)\n",
                "    print(f\"Adversarial Entropy: {adv_entropy:.4f}\")\n",
                "\n",
                "    # Results\n",
                "    delta = adv_entropy - clean_entropy\n",
                "    print(f\"Entropy Delta: {delta:.4f}\")\n",
                "    \n",
                "    # Save simple report\n",
                "    if not os.path.exists(\"results\"):\n",
                "        os.makedirs(\"results\")\n",
                "    with open(\"results/entropy_report.txt\", \"w\") as f:\n",
                "        f.write(f\"Clean Entropy: {clean_entropy:.4f}\\n\")\n",
                "        f.write(f\"Adversarial Entropy: {adv_entropy:.4f}\\n\")\n",
                "        f.write(f\"Delta: {delta:.4f}\\n\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Run Sanity Check\n",
                "!python experiments/sanity_check.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Run Entropy Analysis\n",
                "!python experiments/entropy_analysis.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. Verify Results\n",
                "!cat results/entropy_report.txt"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}